{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a 3D Convolutional Neural Network for hyperspectral data classification\n",
    "\n",
    "In this notebook, you will train and apply a three-dimensional convolutional neural network for classification of hyperspectral data from Luční Hora, Krkonoše mountains, Czechia.\n",
    "\n",
    "Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University\n",
    "\n",
    "Our dataset from Luční Hora is currently not publicly available, but we are working on providing it in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import external libraries:\n",
    "\n",
    "- __torch, torch.nn, torch.optim, torchnet__ - Pytorch related libraries for deep learning\n",
    "- __numpy__ - Arrays to hold our data\n",
    "- __matplotlib.pyplot__ - Draw images\n",
    "- __sklearn.model_selection__ - Cross-validation implemented in scikit-learn\n",
    "- __time.perf_counter__ - Track how long individual functions take to run\n",
    "- __os.path__ - Path manipulation\n",
    "- __tqdm__ - show progress bars during training\n",
    "\n",
    "- __image_preprocessing__ - Our library holding functions for image tiling, preprocessing, etc.\n",
    "- __inference_utils__ - Our library for correctly exporting classifed images\n",
    "- __visualisation_utils__ - Our library for visualising the data\n",
    "\n",
    "Two external libraries are not imported directly in this notebook, but are used by functions in _image_preprocessing_ and _inference_utils_:\n",
    "\n",
    "- __gdal__ - Manipulates spatial data\n",
    "- __scipy.io__ - Reads .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, jaccard_score\n",
    "import torchnet as tnt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "import image_preprocessing\n",
    "import inference_utils\n",
    "import visualisation_utils\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]  \n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill correct paths to your training and reference rasters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS TO TRAINING DATA\n",
    "trainingdata_path = '../data/LH_202008_54bands_9cm.tif'\n",
    "referencedata_path = '../data/LH_202008_reference.tif'\n",
    "\n",
    "trainingdata_path = '../../data/Pavia_centre/Pavia.mat'\n",
    "referencedata_path = '../../data/Pavia_centre/Pavia_gt.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data loading into NumPy\n",
    "Let's start by reading an image into a numpy array, we do this in the background using GDAL.\n",
    "\n",
    "The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1847 by 1563 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_raster = image_preprocessing.read_gdal(trainingdata_path, referencedata_path)\n",
    "loaded_raster = image_preprocessing.read_pavia_centre(trainingdata_path, referencedata_path, out_shape=(1088, 1088, 102))\n",
    "\n",
    "print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_utils.show_img_ref(loaded_raster['imagery'][:, :, [25, 15, 5]], loaded_raster['reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Image tiling\n",
    "We have our data loaded into a numpy array, the next step is to divide the image into individual tiles, which will be the input for our neural network.\n",
    "\n",
    "As we want to perform convolution only in the spatial dimensions, we need to divide the hyperspectral image into tiles of a given shape. Standardly used tile sizes are multiplies of two, for example 2^8 = 256. This tile shape is ensured by setting the variable _tile_shape_ as (256, 256).\n",
    "\n",
    "_overlap_ and _offset_ are not needed for one-dimensional processing.\n",
    "\n",
    "This process creates 143 tiles of 256 by 256 pixels, with the same amount of spectral bands as earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_shape = (64, 64)\n",
    "overlap = 32\n",
    "offset = (0, 0)\n",
    "\n",
    "dataset_tiles = image_preprocessing.tile_training(loaded_raster, tile_shape, overlap, offset)\n",
    "print(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tile filtration\n",
    "However, most of the created tiles do not contain training data, we therefore need to filter them and only keep the tiles with a field-collected reference.\n",
    "\n",
    "This process significantly reduces the size of our dataset from 2 886 861 to 49 842 - training data is available on less than 2 percent of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles, nodata_vals=[65535], is_training=True)\n",
    "print(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}')\n",
    "print(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Data normalization\n",
    "After filtering the tiles to only include training data, we can move onto a final part of the preprocessing - data normalization. In Machine Learning, it is common to normalize all data before classification.\n",
    "\n",
    "The resulting dictionary _preprocessed_tiles_ is subsequently transformed from numpy arrays into pytorch tensors for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tiles, unique, counts = image_preprocessing.normalize_tiles_3d(filtered_tiles, nodata_vals=[65535], is_training=True)\n",
    "print(f'Preprocessed imagery shape {preprocessed_tiles[\"imagery\"].shape}')\n",
    "print(f'Preprocessed reference shape {preprocessed_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tnt.dataset.TensorDataset([preprocessed_tiles['imagery'], preprocessed_tiles['reference']])\n",
    "print(dataset)\n",
    "\n",
    "print(f'Class labels: \\n{unique}\\n')\n",
    "print(f'Number of pixels in a class: \\n{counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network definition\n",
    "After preprocessing our data, we can move onto defining our neural network and functions for training. You can either train your own neural network or use the one we already trained for you (_SpectroSpatialNet_pretrained.pt_). In case you are using the pretrained network, please run only the following code snippet (2.1.) and skip to section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Network structure\n",
    "Our network is named SpectralNet, and its structure is defined in the SpectralNet class, which has three methods:\n",
    "- **__init__** - This method runs automatically when defining an instance of the class, it defines indiviudal layers of the networks (1D convolutions, fully connected layers, maxpooling and also a dropout layer).\n",
    "- **init_weights** - Randomly initialising network weights based on a normal distribution.\n",
    "- **forward** - Defining how data should flow through the network during a forward pass (network structure definition). The PyTorch library automatically creates a method for backward passes based on this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectroSpatialNet(nn.Module):\n",
    "    \"\"\"3D Spectral-Spatial CNN for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the SpectroSpatial model.\n",
    "\n",
    "        n_channels, int, number of input channel\n",
    "        size_e, int list, size of the feature maps of convs for the encoder\n",
    "        size_d, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        # necessary for all classes extending the module class\n",
    "        super(SpectroSpatialNet, self).__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2, 2, return_indices=False)\n",
    "        self.dropout = nn.Dropout3d(p=0.5, inplace=True)\n",
    "\n",
    "        self.n_channels = args['n_channel']\n",
    "        self.size_e = args['size_e']\n",
    "        self.size_d = args['size_d']\n",
    "        self.n_class = args['n_class']\n",
    "\n",
    "        # Encoder layer definitions\n",
    "        def c_en_3d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the encoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv3d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm3d(out_ch), nn.ReLU())\n",
    "\n",
    "        def c_de_2d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the decoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n",
    "\n",
    "        self.c1 = c_en_3d(self.n_channels, self.size_e[0])\n",
    "        self.c2 = c_en_3d(self.size_e[0], self.size_e[1])\n",
    "        self.c3 = c_en_3d(self.size_e[1], self.size_e[2])\n",
    "        self.c4 = c_en_3d(self.size_e[2], self.size_e[3])\n",
    "        self.c5 = c_en_3d(self.size_e[3], self.size_e[4])\n",
    "        self.c6 = c_en_3d(self.size_e[4], self.size_e[5])\n",
    "\n",
    "        self.trans1 = nn.ConvTranspose2d(self.size_d[0], self.size_d[1],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c7 = c_de_2d(self.size_d[1], self.size_d[2])\n",
    "        self.c8 = c_de_2d(self.size_d[2], self.size_d[3])\n",
    "        self.trans2 = nn.ConvTranspose2d(self.size_d[3], self.size_d[4],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c9 = c_de_2d(self.size_d[4], self.size_d[5])\n",
    "        self.c10 = c_de_2d(self.size_d[5], self.size_d[6])\n",
    "\n",
    "        # Final classifying layer\n",
    "        self.classifier = nn.Conv2d(self.size_d[6], self.n_class,\n",
    "                                    1, padding=0)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "\n",
    "        # Put the model on GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def init_weights(self, layer):  # gaussian init for the conv layers\n",
    "        \"\"\"Initialise layer weights.\"\"\"\n",
    "        nn.init.kaiming_normal_(\n",
    "            layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Define model structure.\"\"\"\n",
    "        # Encoder\n",
    "        # level 1\n",
    "        x1 = self.c2(self.c1(input_data))\n",
    "        x2 = self.maxpool(x1)\n",
    "        # level 2\n",
    "        x3 = self.c4(self.c3(x2))\n",
    "        x4 = self.maxpool(x3)\n",
    "        # level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        # Decoder\n",
    "        # Level 3\n",
    "        y5 = torch.flatten(x5, start_dim=1, end_dim=2)\n",
    "        # level 2\n",
    "        y4 = self.trans1(y5)\n",
    "        y3 = self.c8(self.c7(y4))\n",
    "        # level 1\n",
    "        y2 = self.trans2(y3)\n",
    "        y1 = self.c10(self.c9(y2))\n",
    "        # Output\n",
    "        out = self.classifier(self.dropout(y1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Functions for network training\n",
    "Training the network is handled by four functions:\n",
    "- __augment__ - Augments the training data by adding random noise.\n",
    "- __train__ - Trains the network for one epoch. This function contains a for loop, which loads the training data in individual batches. Each batch of training data goes through the network, after which we compute the loss function (cross-entropy). Last step of training is performing an optimiser step, which changes the networks heights.\n",
    "- __eval__ - Evaluates the results on a validation set, should be done periodically during training to check for overfitting.\n",
    "- __train_full__ - Performs the full training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__augment__ takes in the training tile and the corresponding reference labels. It then adds a random value (taken from a normal distribution) at each wavelength and thus slightly modifies the training data. Change _tile_number_ to see the augmentation effect for different tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectroSpatialNet(nn.Module):\n",
    "    \"\"\"3D Spectral-Spatial CNN for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the SpectroSpatial model.\n",
    "\n",
    "        n_channels, int, number of input channel\n",
    "        size_e, int list, size of the feature maps of convs for the encoder\n",
    "        size_d, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        # necessary for all classes extending the module class\n",
    "        super(SpectroSpatialNet, self).__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2, 2, return_indices=False)\n",
    "        self.dropout = nn.Dropout3d(p=0.5, inplace=True)\n",
    "\n",
    "        self.n_channels = args['n_channel']\n",
    "        self.size_e = args['size_e']\n",
    "        self.size_d = args['size_d']\n",
    "        self.n_class = args['n_class']\n",
    "\n",
    "        # Encoder layer definitions\n",
    "        def c_en_3d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the encoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv3d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm3d(out_ch), nn.ReLU())\n",
    "\n",
    "        def c_de_2d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the decoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n",
    "\n",
    "        self.c1 = c_en_3d(self.n_channels, self.size_e[0])\n",
    "        self.c2 = c_en_3d(self.size_e[0], self.size_e[1])\n",
    "        self.c3 = c_en_3d(self.size_e[1], self.size_e[2])\n",
    "        self.c4 = c_en_3d(self.size_e[2], self.size_e[3])\n",
    "        self.c5 = c_en_3d(self.size_e[3], self.size_e[4])\n",
    "        self.c6 = c_en_3d(self.size_e[4], self.size_e[5])\n",
    "\n",
    "        self.trans1 = nn.ConvTranspose2d(self.size_d[0], self.size_d[1],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c7 = c_de_2d(self.size_d[1], self.size_d[2])\n",
    "        self.c8 = c_de_2d(self.size_d[2], self.size_d[3])\n",
    "        self.trans2 = nn.ConvTranspose2d(self.size_d[3], self.size_d[4],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c9 = c_de_2d(self.size_d[4], self.size_d[5])\n",
    "        self.c10 = c_de_2d(self.size_d[5], self.size_d[6])\n",
    "\n",
    "        # Final classifying layer\n",
    "        self.classifier = nn.Conv2d(self.size_d[6], self.n_class,\n",
    "                                    1, padding=0)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "\n",
    "        # Put the model on GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def init_weights(self, layer):  # gaussian init for the conv layers\n",
    "        \"\"\"Initialise layer weights.\"\"\"\n",
    "        nn.init.kaiming_normal_(\n",
    "            layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Define model structure.\"\"\"\n",
    "        # Encoder\n",
    "        # 102-100-98-96 .. 48-46-44-42 .. 21-19-17-15\n",
    "        # 54-52-50-48   .. 24-22-20-18 .. 9-7-5-3\n",
    "        # level 1\n",
    "        x1 = self.c2(self.c1(input_data))\n",
    "        x2 = self.maxpool(x1)\n",
    "        # level 2\n",
    "        x3 = self.c4(self.c3(x2))\n",
    "        x4 = self.maxpool(x3)\n",
    "        # level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        # Decoder\n",
    "        # Level 3\n",
    "        print(x5.shape)\n",
    "        y5 = torch.flatten(x5, start_dim=1, end_dim=2)\n",
    "        print(y5.shape)\n",
    "        # level 2\n",
    "        y4 = self.trans1(y5)\n",
    "        y3 = self.c8(self.c7(y4))\n",
    "        # level 1\n",
    "        y2 = self.trans2(y3)\n",
    "        y1 = self.c10(self.c9(y2))\n",
    "        # Output\n",
    "        out = self.classifier(self.dropout(y1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectroSpatialNet(nn.Module):\n",
    "    \"\"\"3D Spectral-Spatial CNN for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the SpectroSpatial model.\n",
    "\n",
    "        n_channels, int, number of input channel\n",
    "        size_e, int list, size of the feature maps of convs for the encoder\n",
    "        size_d, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        # necessary for all classes extending the module class\n",
    "        super(SpectroSpatialNet, self).__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2, 2, return_indices=False)\n",
    "        self.dropout = nn.Dropout3d(p=0.5, inplace=True)\n",
    "\n",
    "        self.n_channels = args['n_channel']\n",
    "        self.size_e = args['size_e']\n",
    "        self.size_d = args['size_d']\n",
    "        self.n_class = args['n_class']\n",
    "\n",
    "        # Encoder layer definitions\n",
    "        def c_en_3d(in_ch, out_ch, k_size=3, pad=(0,1,1), pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the encoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv3d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm3d(out_ch), nn.ReLU())\n",
    "\n",
    "        def c_de_2d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the decoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n",
    "\n",
    "        self.c1 = c_en_3d(self.n_channels, self.size_e[0])\n",
    "        self.c2 = c_en_3d(self.size_e[0], self.size_e[1])\n",
    "        self.c3 = c_en_3d(self.size_e[1], self.size_e[2])\n",
    "        \n",
    "        self.c4 = c_en_3d(self.size_e[2], self.size_e[3])\n",
    "        self.c5 = c_en_3d(self.size_e[3], self.size_e[4])\n",
    "        self.c6 = c_en_3d(self.size_e[4], self.size_e[5])\n",
    "        \n",
    "        self.c7 = c_en_3d(self.size_e[5], self.size_e[6])\n",
    "        self.c8 = c_en_3d(self.size_e[6], self.size_e[7])\n",
    "        self.c9 = c_en_3d(self.size_e[7], self.size_e[8])\n",
    "\n",
    "        self.trans1 = nn.ConvTranspose2d(self.size_d[0], self.size_d[1],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c10 = c_de_2d(self.size_d[1], self.size_d[2])\n",
    "        self.c11 = c_de_2d(self.size_d[2], self.size_d[3])\n",
    "        self.c12 = c_de_2d(self.size_d[3], self.size_d[4])\n",
    "        self.trans2 = nn.ConvTranspose2d(self.size_d[4], self.size_d[5],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        \n",
    "        self.c13 = c_de_2d(self.size_d[5], self.size_d[6])\n",
    "        self.c14 = c_de_2d(self.size_d[6], self.size_d[7])\n",
    "        self.c15 = c_de_2d(self.size_d[7], self.size_d[8])\n",
    "\n",
    "        # Final classifying layer\n",
    "        self.classifier = nn.Conv2d(self.size_d[8], self.n_class,\n",
    "                                    1, padding=0)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.c11[0].apply(self.init_weights)\n",
    "        self.c12[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c13[0].apply(self.init_weights)\n",
    "        self.c14[0].apply(self.init_weights)\n",
    "        self.c15[0].apply(self.init_weights)\n",
    "\n",
    "        self.classifier.apply(self.init_weights)\n",
    "\n",
    "        # Put the model on GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def init_weights(self, layer):  # gaussian init for the conv layers\n",
    "        \"\"\"Initialise layer weights.\"\"\"\n",
    "        nn.init.kaiming_normal_(\n",
    "            layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Define model structure.\"\"\"\n",
    "        # Encoder\n",
    "        # 102-100-98-96 .. 48-46-44-42 .. 21-19-17-15\n",
    "        # 54-52-50-48   .. 24-22-20-18 .. 9-7-5-3\n",
    "        # level 1\n",
    "        x1 = self.c3(self.c2(self.c1(input_data)))\n",
    "        x2 = self.maxpool(x1)\n",
    "        # level 2\n",
    "        x3 = self.c6(self.c5(self.c4(x2)))\n",
    "        x4 = self.maxpool(x3)\n",
    "        # level 3\n",
    "        x5 = self.c9(self.c8(self.c7(x4)))\n",
    "        # Decoder\n",
    "        # Level 3\n",
    "        y5 = torch.flatten(x5, start_dim=1, end_dim=2)\n",
    "        # level 2\n",
    "        y4 = self.trans1(y5)\n",
    "        y3 = self.c12(self.c11(self.c10(y4)))\n",
    "        # level 1\n",
    "        y2 = self.trans2(y3)\n",
    "        y1 = self.c15(self.c14(self.c13(y2)))\n",
    "        # Output\n",
    "        out = self.classifier(self.dropout(y1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(obs, g_t):\n",
    "    \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"\n",
    "    sigma, clip= 0.01, 0.03 \n",
    "\n",
    "    # Random noise\n",
    "    rand = torch.clamp(torch.mul(sigma, torch.randn([1, 1, 102, tile_shape[0],tile_shape[1]])), -clip, clip)\n",
    "    obs = torch.add(obs, rand)\n",
    "\n",
    "    # Random rotation 0 90 180 270 degree\n",
    "    n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3\n",
    "    obs = torch.rot90(obs, n_turn, dims=(3,4))\n",
    "    g_t = torch.rot90(g_t, n_turn, dims=(1,2))\n",
    "\n",
    "    return obs, g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_number = 46\n",
    "visualisation_utils.show_augment_spectro_spatial(preprocessed_tiles, tile_number, augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train__ trains the network for one epoch. This function contains a for loop, which loads the training data in individual batches. Each batch of training data goes through the network, after which we compute the loss function (cross-entropy). Last step of training is performing an optimiser step, which changes the networks heights.\n",
    "\n",
    "__eval__ evaluates the results on a validation set, should be done periodically during training to check for overfitting.\n",
    "\n",
    "__train_full__ performs the full training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train() #switch the model in training mode\n",
    "  \n",
    "    #the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    #will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad() #put gradient to zero\n",
    "\n",
    "        tiles, gt = augment(tiles, gt)\n",
    "\n",
    "        pred = model(tiles.cuda()) #compute the prediction\n",
    "\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])\n",
    "        loss.backward() #compute gradients\n",
    "\n",
    "        for p in model.parameters(): #we clip the gradient at norm 1\n",
    "            p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "\n",
    "        optimizer.step() #one SGD step\n",
    "        loss_meter.add(loss.item())\n",
    "        \n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "def eval(model, sampler):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "  \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            pred = model(tiles.cuda())\n",
    "            loss = nn.functional.cross_entropy(pred.cpu(), gt)\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "\n",
    "    #initialize the model\n",
    "    model = SpectroSpatialNet(args)\n",
    "\n",
    "    print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')\n",
    "  \n",
    "    #define the Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],\n",
    "                                               gamma=args['scheduler_gamma'])\n",
    "  \n",
    "    train_loss = np.empty(args['n_epoch'])\n",
    "    test_epochs = []\n",
    "    test_loss = []\n",
    "\n",
    "    for i_epoch in range(args['n_epoch']):\n",
    "        #train one epoch\n",
    "        print(f'Epoch #{str(i_epoch+1)}')\n",
    "        train_loss[i_epoch] = train(model, optimizer, args)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Periodic testing on the validation set\n",
    "        if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):\n",
    "            print('Evaluation')\n",
    "            loss_test = eval(model, args['test_subsampler'])\n",
    "            test_epochs.append(i_epoch + 1)\n",
    "            test_loss.append(loss_test)\n",
    "            \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')\n",
    "    plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')\n",
    "    plt.plot(test_epochs, test_loss, label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    args['loss_test'] = test_loss[-1]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hyperparameter definition\n",
    "Training networks requires first setting several hyperparameters, please feel free to play around with them and try different values for the number of training epochs, learning rate or batch size.\n",
    "\n",
    "- __n_channel__ - number of channels, set to 1 for our task\n",
    "- __n_class__ - number of classification classes\n",
    "- __size_e__ - number of filters in each NN layer of the encoder\n",
    "- __size_d__ - number of filters in each NN layer of the decoder\n",
    "- __crossval_nfolds__ - Number of folds for crossvalidation\n",
    "- __n_epoch_test__ - after how many training epochs do we validate on the validation set\n",
    "- __scheduler_milestones__ - after how many epochs do we reduce the training rate\n",
    "- __scheduler_gamma__ - by what factor do we reduce the training rate\n",
    "- __class_weights__ - training weights for individual classes, used to offset imbalanced class distribution\n",
    "\n",
    "- __n_epoch__ - how many epochs are performed during training\n",
    "- __lr__ - how fast can individual network parameters change during one training epoch\n",
    "- __batch_size__ - how many tiles should be included in each gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args = { #Dict to store all model parameters\n",
    "    'n_channel': 1,\n",
    "    'n_class': len(unique),\n",
    "    'size_e': [16,16,16,16,16,16,32,32,32],\n",
    "    'size_d': [480,32,32,32,32,32,32,32,32,32],\n",
    "    \n",
    "    'crossval_nfolds': 3,\n",
    "    'n_epoch_test': 2,          #periodicity of evaluation on test set\n",
    "    'scheduler_milestones': [60,80,90],\n",
    "    'scheduler_gamma': 0.3,\n",
    "    'class_weights': torch.tensor([0.0, 0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),\n",
    "\n",
    "    'n_epoch': 5,\n",
    "    'lr': 1e-6,\n",
    "    'batch_size': 4,\n",
    "}\n",
    "model_save_folder = '../../models/Pavia/3D'\n",
    "\n",
    "print(f'''Number of models to be trained:\n",
    "    {args['crossval_nfolds']}\n",
    "Number of spectral channels:\n",
    "    {args['n_channel']}\n",
    "Initial learning rate:\n",
    "    {args['lr']}\n",
    "Batch size:\n",
    "    {args['batch_size']}\n",
    "Number of training epochs:\n",
    "    {args['n_epoch']}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training a 3D network\n",
    "kfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True)\n",
    "trained_models = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'Training starts for model number {str(fold+1)}')\n",
    "    \n",
    "    a = perf_counter()\n",
    "    args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    trained_models.append((train_full(args), args['loss_test']))\n",
    "    \n",
    "    state_dict_path = join(model_save_folder, f'fold_{str(fold)}.pt')\n",
    "    torch.save(trained_models[fold][0].state_dict(), state_dict_path)\n",
    "    print(f'Model saved to: {state_dict_path}')\n",
    "    print(f'Training finished in {str(perf_counter()-a)}s')\n",
    "    print('\\n\\n')\n",
    "\n",
    "print(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}')\n",
    "print(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model definition\n",
    "args = {\n",
    "    'n_class': 10,\n",
    "    'n_channel': 1,\n",
    "    'size_e': [16,16,16,16,16,16,32,32,32],\n",
    "    'size_d': [480,32,32,32,32,32,32,32,32,32],\n",
    "}\n",
    "# Path to the state_dictionary\n",
    "state_dict_path = '../../models/Pavia/3D/fold_0.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SpectroSpatialNet(args)\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '../../data/LH_202008_54bands_9cm.tif'\n",
    "source_path = '../../data/Pavia_centre/Pavia.mat'\n",
    "\n",
    "tile_shape = (128, 128)\n",
    "overlap = 64\n",
    "offset_topleft = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "#raster_orig = image_preprocessing.read_gdal_with_geoinfo(source_path, offset_topleft)\n",
    "raster_orig = image_preprocessing.read_pavia_centre(source_path, out_shape=(1088, 1088, 102))\n",
    "\n",
    "dataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'], out_shape=tile_shape, \n",
    "                                                    out_overlap=overlap, offset=offset_topleft)\n",
    "dataset_full = image_preprocessing.normalize_tiles_3d(dataset_full_tiles, nodata_vals=[0])\n",
    "dataset = tnt.dataset.TensorDataset(dataset_full['imagery'])\n",
    "end = perf_counter()\n",
    "print(f'Loading the imagery took {end - start} seconds.')\n",
    "\n",
    "print(dataset_full_tiles['imagery'].shape)\n",
    "print(dataset_full_tiles['dimensions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Applying the CNN and exporting results\n",
    "The following snippet applies the CNN and exports the resulting classified raster into output_path for further analysis (e.g. validation in GIS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../results/test_result_3d.tif'\n",
    "\n",
    "start = perf_counter()\n",
    "arr_class = inference_utils.combine_tiles_2d(model, dataset, tile_shape, overlap, dataset_full_tiles['dimensions'])\n",
    "print(np.unique(arr_class, return_counts=True))\n",
    "inference_utils.export_result(output_path, arr_class, raster_orig['geoinfo'])\n",
    "print(f'The processing took {perf_counter() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualise the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n",
    "                                    loaded_raster['reference'], arr_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
