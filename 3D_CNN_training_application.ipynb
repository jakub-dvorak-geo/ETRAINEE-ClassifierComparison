{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import perf_counter\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, jaccard_score\n",
    "import torchnet as tnt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "import image_preprocessing\n",
    "import inference_utils\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]  \n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS TO TRAINING DATA\n",
    "trainingdata_path = r'C:\\Users\\dd\\Documents\\NATUR_CUNI\\additional_projects\\prace\\ETRAINEE\\data\\LH_202008_54bands_9cm.tif'\n",
    "referencedata_path = r'C:\\Users\\dd\\Documents\\NATUR_CUNI\\additional_projects\\prace\\ETRAINEE\\data\\LH_202008_reference.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_raster = image_preprocessing.read_gdal(trainingdata_path, referencedata_path)\n",
    "print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_shape = (128, 128)\n",
    "overlap = 64\n",
    "offset = (0, 0)\n",
    "\n",
    "dataset_tiles = image_preprocessing.tile_training(loaded_raster, tile_shape, overlap, offset)\n",
    "print(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles, nodata_vals=[65535], is_training=True)\n",
    "print(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}')\n",
    "print(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tiles, unique, counts = image_preprocessing.normalize_tiles_3d(filtered_tiles, nodata_vals=[65535], is_training=True)\n",
    "print(f'Preprocessed imagery shape {preprocessed_tiles[\"imagery\"].shape}')\n",
    "print(f'Preprocessed reference shape {preprocessed_tiles[\"reference\"].shape}')\n",
    "\n",
    "dataset = tnt.dataset.TensorDataset([preprocessed_tiles['imagery'], preprocessed_tiles['reference']])\n",
    "print(dataset)\n",
    "\n",
    "print(f'Class labels: \\n{unique}\\n')\n",
    "print(f'Number of pixels in a class: \\n{counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectroSpatialNet(nn.Module):\n",
    "    \"\"\"3D Spectral-Spatial CNN for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the SpectroSpatial model.\n",
    "\n",
    "        n_channels, int, number of input channel\n",
    "        size_e, int list, size of the feature maps of convs for the encoder\n",
    "        size_d, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        # necessary for all classes extending the module class\n",
    "        super(SpectroSpatialNet, self).__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2, 2, return_indices=False)\n",
    "        self.dropout = nn.Dropout3d(p=0.5, inplace=True)\n",
    "\n",
    "        self.n_channels = args['n_channel']\n",
    "        self.size_e = args['size_e']\n",
    "        self.size_d = args['size_d']\n",
    "        self.n_class = args['n_class']\n",
    "\n",
    "        # Encoder layer definitions\n",
    "        def c_en_3d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the encoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv3d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm3d(out_ch), nn.ReLU())\n",
    "\n",
    "        def c_de_2d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n",
    "                    bias=False):\n",
    "            \"\"\"Create default conv layer for the decoder.\"\"\"\n",
    "            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=pad, padding_mode=pad_mode,\n",
    "                                           bias=bias),\n",
    "                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n",
    "\n",
    "        self.c1 = c_en_3d(self.n_channels, self.size_e[0])\n",
    "        self.c2 = c_en_3d(self.size_e[0], self.size_e[1])\n",
    "        self.c3 = c_en_3d(self.size_e[1], self.size_e[2])\n",
    "        self.c4 = c_en_3d(self.size_e[2], self.size_e[3])\n",
    "        self.c5 = c_en_3d(self.size_e[3], self.size_e[4])\n",
    "        self.c6 = c_en_3d(self.size_e[4], self.size_e[5])\n",
    "\n",
    "        self.trans1 = nn.ConvTranspose2d(self.size_d[0], self.size_d[1],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c7 = c_de_2d(self.size_d[1], self.size_d[2])\n",
    "        self.c8 = c_de_2d(self.size_d[2], self.size_d[3])\n",
    "        self.trans2 = nn.ConvTranspose2d(self.size_d[3], self.size_d[4],\n",
    "                                      kernel_size=2, stride=2)\n",
    "        self.c9 = c_de_2d(self.size_d[4], self.size_d[5])\n",
    "        self.c10 = c_de_2d(self.size_d[5], self.size_d[6])\n",
    "\n",
    "        # Final classifying layer\n",
    "        self.classifier = nn.Conv2d(self.size_d[6], self.n_class,\n",
    "                                    1, padding=0)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "\n",
    "        # Put the model on GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def init_weights(self, layer):  # gaussian init for the conv layers\n",
    "        \"\"\"Initialise layer weights.\"\"\"\n",
    "        nn.init.kaiming_normal_(\n",
    "            layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Define model structure.\"\"\"\n",
    "        # Encoder\n",
    "        # level 1\n",
    "        x1 = self.c2(self.c1(input_data))\n",
    "        x2 = self.maxpool(x1)\n",
    "        # level 2\n",
    "        x3 = self.c4(self.c3(x2))\n",
    "        x4 = self.maxpool(x3)\n",
    "        # level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        # Decoder\n",
    "        # Level 3\n",
    "        y5 = torch.flatten(x5, start_dim=1, end_dim=2)\n",
    "        # level 2\n",
    "        y4 = self.trans1(y5)\n",
    "        y3 = self.c8(self.c7(y4))\n",
    "        # level 1\n",
    "        y2 = self.trans2(y3)\n",
    "        y1 = self.c10(self.c9(y2))\n",
    "        # Output\n",
    "        out = self.classifier(self.dropout(y1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(obs, g_t):\n",
    "    \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"\n",
    "    sigma, clip= 0.01, 0.03 \n",
    "\n",
    "    # Random noise\n",
    "    rand = torch.clamp(torch.mul(sigma, torch.randn([1, 1, 54, tile_shape[0],tile_shape[1]])), -clip, clip)\n",
    "    obs = torch.add(obs, rand)\n",
    "\n",
    "    # Random rotation 0 90 180 270 degree\n",
    "    n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3\n",
    "    obs = torch.rot90(obs, n_turn, dims=(3,4))\n",
    "    g_t = torch.rot90(g_t, n_turn, dims=(1,2))\n",
    "\n",
    "    return obs, g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train() #switch the model in training mode\n",
    "  \n",
    "    #the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    #will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad() #put gradient to zero\n",
    "\n",
    "        tiles, gt = augment(tiles, gt)\n",
    "\n",
    "        pred = model(tiles.cuda()) #compute the prediction\n",
    "\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])\n",
    "        loss.backward() #compute gradients\n",
    "\n",
    "        for p in model.parameters(): #we clip the gradient at norm 1\n",
    "            p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "\n",
    "        optimizer.step() #one SGD step\n",
    "        loss_meter.add(loss.item())\n",
    "        \n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "def eval(model, sampler):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "  \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            pred = model(tiles.cuda())\n",
    "            loss = nn.functional.cross_entropy(pred.cpu(), gt)\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "\n",
    "    #initialize the model\n",
    "    model = SpectroSpatialNet(args)\n",
    "\n",
    "    print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')\n",
    "  \n",
    "    #define the Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],\n",
    "                                               gamma=args['scheduler_gamma'])\n",
    "  \n",
    "    train_loss = np.empty(args['n_epoch'])\n",
    "    test_epochs = []\n",
    "    test_loss = []\n",
    "\n",
    "    for i_epoch in range(args['n_epoch']):\n",
    "        #train one epoch\n",
    "        print(f'Epoch #{str(i_epoch+1)}')\n",
    "        train_loss[i_epoch] = train(model, optimizer, args)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Periodic testing on the validation set\n",
    "        if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):\n",
    "            print('Evaluation')\n",
    "            loss_test = eval(model, args['test_subsampler'])\n",
    "            test_epochs.append(i_epoch + 1)\n",
    "            test_loss.append(loss_test)\n",
    "            \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')\n",
    "    plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')\n",
    "    plt.plot(test_epochs, test_loss, label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    args['loss_test'] = test_loss[-1]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a 3D network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = { #Dict to store all model parameters\n",
    "    'n_channel': 1,\n",
    "    'n_class': len(unique),\n",
    "    'size_e': [16,16,32,32,32,32],\n",
    "    'size_d': [416,64,64,64,64,32,32],\n",
    "    \n",
    "    'crossval_nfolds': 3,\n",
    "    'n_epoch_test': 2,          #periodicity of evaluation on test set\n",
    "    'scheduler_milestones': [60,80,90],\n",
    "    'scheduler_gamma': 0.3,\n",
    "    'class_weights': torch.tensor([0.0, 0.202, 0.341, 0.033, 0.16, 0.14, 0.03, 0.013, 0.023, 0.058]),\n",
    "\n",
    "    'n_epoch': 100,\n",
    "    'lr': 5e-6,\n",
    "    'batch_size': 1,\n",
    "}\n",
    "model_save_folder = r'C:\\Users\\dd\\Documents\\NATUR_CUNI\\additional_projects\\prace\\ETRAINEE\\data\\models\\3D'\n",
    "\n",
    "print(f'''Number of models to be trained:\n",
    "    {args['crossval_nfolds']}\n",
    "Number of spectral channels:\n",
    "    {args['n_channel']}\n",
    "Initial learning rate:\n",
    "    {args['lr']}\n",
    "Batch size:\n",
    "    {args['batch_size']}\n",
    "Number of training epochs:\n",
    "    {args['n_epoch']}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training a 3D network\n",
    "kfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True)\n",
    "trained_models = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'Training starts for model number {str(fold+1)}')\n",
    "    \n",
    "    a = perf_counter()\n",
    "    args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    trained_models.append((train_full(args), args['loss_test']))\n",
    "    \n",
    "    state_dict_path = os.path.join(model_save_folder, f'fold_{str(fold)}.pt')\n",
    "    torch.save(trained_models[fold][0].state_dict(), state_dict_path)\n",
    "    print(f'Model saved to: {state_dict_path}')\n",
    "    print(f'Training finished in {str(perf_counter()-a)}s')\n",
    "    print('\\n\\n')\n",
    "\n",
    "print(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}')\n",
    "print(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reusing a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the state_dictionary\n",
    "state_dict_path = r'C:\\Users\\dd\\Documents\\NATUR_CUNI\\additional_projects\\prace\\ETRAINEE\\data\\models\\fold_2.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a model to state_dict_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save a trained model state_dictionary\n",
    "torch.save(trained_model.state_dict(), state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse a model at state_dict_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters for model definition\n",
    "\n",
    "args = {} #stores the parameters\n",
    "args['n_class'] = 10\n",
    "args['n_channel'] = 1\n",
    "args['size_e'] =  np.divide([64,64,128,128,256,256,512,512,1024,1024],        32).astype(np.int32)\n",
    "args['size_d'] = np.divide(np.multiply([1024,512,512,512,256,256,256,128,128,128,64,64], 3), 32).astype(np.int32)\n",
    "args['cuda'] = True\n",
    "\n",
    "\n",
    "# Load a trained model state_dictionary\n",
    "#state_dict_path = 'd:/studenti/JD/ETRAINEE/models/fold_0.pt'\n",
    "model = SpectroSpatialNet(args)\n",
    "#state_dict_path = 'D:/Studenti/JD/Jeseniky_2020/models/KrakonosNet_jes_1e-3_200epochs.pt'\n",
    "#model = KrakonosNet(args['n_channel'], args['size_e'], args['size_d'], args['n_class'], args['cuda'])\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results\n",
    "Results are not georeferenced â€“ use ArcPy_georeference_results.py for georeferencing and combining into a single raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = r'C:\\Users\\dd\\Documents\\NATUR_CUNI\\additional_projects\\prace\\ETRAINEE\\data\\LH_202008_54pasem_9cm.tif'\n",
    "tile_shape = (256, 256)\n",
    "overlap = 128\n",
    "offset_topleft = (0, 0)\n",
    "\n",
    "start = perf_counter()\n",
    "raster_orig = image_preprocessing.read_gdal_with_geoinfo(source_path, offset_topleft)\n",
    "print(raster_orig['geoinfo'])\n",
    "\n",
    "dataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'], out_shape=tile_shape, \n",
    "                                                    out_overlap=overlap, offset=offset_topleft)\n",
    "dataset_full = image_preprocessing.normalize_tiles_3d(dataset_full_tiles, nodata_vals=[0])\n",
    "dataset = tnt.dataset.TensorDataset(dataset_full['imagery'])\n",
    "end = perf_counter()\n",
    "print(f'Loading the imagery took {end - start} seconds.')\n",
    "\n",
    "print(dataset_full_tiles['imagery'].shape)\n",
    "print(dataset_full_tiles['dimensions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'C:\\Users\\dd\\Documents\\NATUR_CUNI\\additional_projects\\prace\\ETRAINEE\\data\\test_result_3d.tif'\n",
    "\n",
    "start = perf_counter()\n",
    "arr_class = inference_utils.combine_tiles_2d(model, dataset, tile_shape, overlap, dataset_full_tiles['dimensions'])\n",
    "print(np.unique(arr_class, return_counts=True))\n",
    "inference_utils.export_result(output_path, arr_class, raster_orig['geoinfo'])\n",
    "end = perf_counter()\n",
    "print(f'The processing took {end - start} seconds.')\n",
    "plt.imshow(arr_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
